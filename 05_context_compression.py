"""
ç¬¬5ç« ï¼šä¸Šä¸‹æ–‡å‹ç¼© - 8æ®µå¼å‹ç¼©ç®—æ³•

å®ç° Claude Code çš„ä¸Šä¸‹æ–‡å‹ç¼©æœºåˆ¶
"""
import os
from typing import TypedDict, List, Annotated, Literal
from datetime import datetime

# LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# LangChain
from langchain_core.tools import tool
from langchain_core.messages import (
    HumanMessage,
    SystemMessage,
    AIMessage,
    BaseMessage,
    RemoveMessage,
    ToolMessage
)

# LLM
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi


# ============================================================================
# 1. é…ç½®
# ============================================================================

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "chapter-05-context-compression"

# åˆå§‹åŒ–LLM
# llm = ChatOpenAI(model="gpt-5", temperature=0)
llm = ChatTongyi(model="qwen-max", temperature=0)
print(f"âœ… LLMåˆå§‹åŒ–æˆåŠŸ: {llm.model_name}")


# ============================================================================
# 2. Token ç›‘æ§æœºåˆ¶
# ============================================================================

def get_latest_token_usage(messages: List[BaseMessage]) -> int:
    """å€’åºæŸ¥æ‰¾æœ€æ–°çš„tokenä½¿ç”¨æƒ…å†µ

    è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å®ç°ï¼Œé¿å…éå†æ‰€æœ‰æ¶ˆæ¯ã€‚
    ä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹æŸ¥æ‰¾ï¼Œå› ä¸ºtoken usageä¿¡æ¯é€šå¸¸åœ¨æœ€è¿‘çš„AIMessageä¸­ã€‚

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        æœ€æ–°çš„tokenä½¿ç”¨æ€»æ•°
    """
    # å€’åºæ‰«æï¼Œæ‰¾åˆ°æœ€æ–°çš„usageä¿¡æ¯
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and hasattr(msg, 'response_metadata'):
            usage = msg.response_metadata.get('token_usage', {})

            if usage:
                # ç²¾ç¡®è®¡ç®—ï¼šåŒ…å«æ‰€æœ‰tokenç±»å‹
                total = usage.get('total_tokens', 0)
                if total > 0:
                    return total

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°usageä¿¡æ¯ï¼Œè¿”å›0
    return 0


def estimate_tokens(messages: List[BaseMessage]) -> int:
    """ä¼°ç®—tokenæ•°é‡ï¼ˆå½“æ²¡æœ‰ç²¾ç¡®token_usageæ—¶ä½¿ç”¨ï¼‰

    ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰æˆ– 1.5 ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        ä¼°ç®—çš„tokenæ€»æ•°
    """
    total_chars = 0

    for msg in messages:
        if hasattr(msg, 'content') and msg.content:
            total_chars += len(str(msg.content))

    # ä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼ˆå‡è®¾ä¸­è‹±æ–‡æ··åˆï¼‰
    return int(total_chars / 3)


# ============================================================================
# 3. å‹ç¼©åˆ¤æ–­é€»è¾‘
# ============================================================================

TOKEN_THRESHOLD = 0.92  # 92% é˜ˆå€¼

def needs_compression(
    messages: List[BaseMessage],
    max_tokens: int = 128000,  # Claude Sonnet 3.5ä¸Šä¸‹æ–‡çª—å£
    reserved_output: int = 4000  # ä¸ºè¾“å‡ºé¢„ç•™çš„token
) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©

    ç»¼åˆè€ƒè™‘ï¼š
    1. å½“å‰tokenä½¿ç”¨é‡
    2. æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å¤§å°
    3. é¢„ç•™çš„è¾“å‡ºç©ºé—´

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        max_tokens: æœ€å¤§tokenæ•°
        reserved_output: ä¸ºè¾“å‡ºé¢„ç•™çš„tokenæ•°

    Returns:
        æ˜¯å¦éœ€è¦å‹ç¼©
    """
    threshold = TOKEN_THRESHOLD
    # è·å–å½“å‰tokenä½¿ç”¨é‡
    current_tokens = get_latest_token_usage(messages)

    # å¦‚æœæ²¡æœ‰ç²¾ç¡®çš„usageä¿¡æ¯ï¼Œä½¿ç”¨ä¼°ç®—
    if current_tokens == 0:
        current_tokens = estimate_tokens(messages)

    # è®¡ç®—å¯ç”¨tokenï¼ˆå‡å»é¢„ç•™çš„è¾“å‡ºç©ºé—´ï¼‰
    available_tokens = max_tokens - reserved_output

    # è®¡ç®—ä½¿ç”¨ç‡
    usage_ratio = current_tokens / available_tokens

    print(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
    print(f"   å½“å‰: {current_tokens:,} tokens")
    print(f"   å¯ç”¨: {available_tokens:,} tokens")
    print(f"   ä½¿ç”¨ç‡: {usage_ratio:.1%}")
    print(f"   é˜ˆå€¼: {threshold:.1%}")

    return usage_ratio > threshold


# ============================================================================
# 4. 8æ®µå¼å‹ç¼©æç¤ºè¯
# ============================================================================

COMPRESSION_PROMPT = """ä½ çš„ä»»åŠ¡æ˜¯åˆ›å»ºåˆ°ç›®å‰ä¸ºæ­¢å¯¹è¯çš„è¯¦ç»†æ‘˜è¦ï¼Œå¯†åˆ‡å…³æ³¨ç”¨æˆ·çš„æ˜ç¡®è¯·æ±‚å’Œä½ ä¹‹å‰çš„è¡ŒåŠ¨ã€‚
æ­¤æ‘˜è¦åº”å½»åº•æ•è·æŠ€æœ¯ç»†èŠ‚ã€ä»£ç æ¨¡å¼å’Œæ¶æ„å†³ç­–ï¼Œè¿™äº›å¯¹äºåœ¨ä¸ä¸¢å¤±ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç»§ç»­å¼€å‘å·¥ä½œè‡³å…³é‡è¦ã€‚

åœ¨æä¾›æœ€ç»ˆæ‘˜è¦ä¹‹å‰ï¼Œå°†ä½ çš„åˆ†æåŒ…è£…åœ¨<analysis>æ ‡ç­¾ä¸­ï¼Œä»¥ç»„ç»‡ä½ çš„æ€è€ƒå¹¶ç¡®ä¿ä½ æ¶µç›–äº†æ‰€æœ‰å¿…è¦çš„è¦ç‚¹ã€‚

ä½ çš„æ‘˜è¦åº”åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†:

1. **ä¸»è¦è¯·æ±‚å’Œæ„å›¾**: è¯¦ç»†æ•è·ç”¨æˆ·çš„æ‰€æœ‰æ˜ç¡®è¯·æ±‚å’Œæ„å›¾
   - ç”¨æˆ·æ˜ç¡®è¯´äº†ä»€ä¹ˆï¼Ÿ
   - ä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
   - æœ‰å“ªäº›éšå«çš„éœ€æ±‚ï¼Ÿ

2. **å…³é”®æŠ€æœ¯æ¦‚å¿µ**: åˆ—å‡ºè®¨è®ºçš„æ‰€æœ‰é‡è¦æŠ€æœ¯æ¦‚å¿µã€æŠ€æœ¯å’Œæ¡†æ¶
   - ä½¿ç”¨äº†å“ªäº›æŠ€æœ¯æ ˆï¼Ÿ
   - æ¶‰åŠå“ªäº›æ ¸å¿ƒæ¦‚å¿µï¼Ÿ
   - æœ‰å“ªäº›é‡è¦çš„æ¶æ„å†³ç­–ï¼Ÿ

3. **æ–‡ä»¶å’Œä»£ç æ®µ**: æšä¸¾æ£€æŸ¥ã€ä¿®æ”¹æˆ–åˆ›å»ºçš„ç‰¹å®šæ–‡ä»¶å’Œä»£ç æ®µ
   - ç‰¹åˆ«æ³¨æ„æœ€è¿‘çš„æ¶ˆæ¯
   - åŒ…å«å®Œæ•´çš„ä»£ç ç‰‡æ®µï¼ˆå¦‚é€‚ç”¨ï¼‰
   - è®°å½•æ–‡ä»¶è·¯å¾„å’Œå…³é”®è¡Œå·

4. **é”™è¯¯å’Œä¿®å¤**: åˆ—å‡ºä½ é‡åˆ°çš„æ‰€æœ‰é”™è¯¯ä»¥åŠä¿®å¤æ–¹æ³•
   - å…·ä½“çš„é”™è¯¯ä¿¡æ¯
   - è§£å†³æ–¹æ¡ˆå’ŒåŸå› 
   - ç‰¹åˆ«æ³¨æ„ç”¨æˆ·çš„åé¦ˆ

5. **é—®é¢˜è§£å†³**: è®°å½•å·²è§£å†³çš„é—®é¢˜å’Œä»»ä½•æ­£åœ¨è¿›è¡Œçš„æ•…éšœæ’é™¤å·¥ä½œ
   - è§£å†³äº†å“ªäº›éš¾é¢˜ï¼Ÿ
   - é‡‡ç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼Ÿ
   - è¿˜æœ‰å“ªäº›æœªè§£å†³çš„é—®é¢˜ï¼Ÿ

6. **æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯**: åˆ—å‡ºæ‰€æœ‰éå·¥å…·ç»“æœçš„ç”¨æˆ·æ¶ˆæ¯
   - è¿™äº›å¯¹äºç†è§£ç”¨æˆ·çš„åé¦ˆå’Œå˜åŒ–çš„æ„å›¾è‡³å…³é‡è¦
   - æŒ‰æ—¶é—´é¡ºåºåˆ—å‡º
   - æ³¨æ„ç”¨æˆ·æ€åº¦å’Œéœ€æ±‚çš„å˜åŒ–

7. **å¾…å¤„ç†ä»»åŠ¡**: æ¦‚è¿°ä½ æ˜ç¡®è¢«è¦æ±‚å¤„ç†çš„ä»»ä½•å¾…å¤„ç†ä»»åŠ¡
   - å“ªäº›ä»»åŠ¡è¿˜æ²¡å®Œæˆï¼Ÿ
   - ä¼˜å…ˆçº§å¦‚ä½•ï¼Ÿ
   - æœ‰å“ªäº›ä¾èµ–å…³ç³»ï¼Ÿ

8. **å½“å‰å·¥ä½œ**: è¯¦ç»†æè¿°åœ¨æ­¤æ‘˜è¦è¯·æ±‚ä¹‹å‰æ­£åœ¨è¿›è¡Œçš„ç¡®åˆ‡å·¥ä½œ
   - æœ€ååœ¨åšä»€ä¹ˆï¼Ÿ
   - è¿›å±•åˆ°å“ªä¸€æ­¥äº†ï¼Ÿ
   - ä¸‹ä¸€æ­¥è®¡åˆ’åšä»€ä¹ˆï¼Ÿ

9. **å¯é€‰çš„ä¸‹ä¸€æ­¥**: åˆ—å‡ºä¸ä½ æœ€è¿‘æ­£åœ¨åšçš„å·¥ä½œç›¸å…³çš„ä¸‹ä¸€æ­¥
   - é€»è¾‘ä¸Šçš„ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ
   - æœ‰å“ªäº›å¯èƒ½çš„æ–¹å‘ï¼Ÿ

è¯·ç¡®ä¿æ‘˜è¦è¶³å¤Ÿè¯¦ç»†ï¼Œä½¿å¾—å¦ä¸€ä¸ªAIåŠ©æ‰‹ï¼ˆæˆ–ä½ è‡ªå·±åœ¨æ–°ä¼šè¯ä¸­ï¼‰èƒ½å¤Ÿæ— ç¼åœ°ç»§ç»­è¿™ä¸ªå¯¹è¯å’Œå·¥ä½œã€‚
"""


# ============================================================================
# 5. çŠ¶æ€å®šä¹‰
# ============================================================================

class CompressionRecord(TypedDict):
    """å‹ç¼©è®°å½•"""
    timestamp: str
    messages_removed: int
    messages_kept: int
    tokens_before: int
    tokens_after: int
    summary_content: str


class AgentState(TypedDict):
    """æ‰©å±•çš„AgentçŠ¶æ€

    åŒ…å«ï¼š
    - messages: å¯¹è¯å†å²
    - compression_history: å‹ç¼©å†å²
    """
    messages: Annotated[List[BaseMessage], add_messages]
    compression_history: List[CompressionRecord]  # ä¸éœ€è¦ reducerï¼Œç›´æ¥æ›¿æ¢


# ============================================================================
# 6. å‹ç¼©èŠ‚ç‚¹å®ç°
# ============================================================================

def compression_node(state: AgentState) -> dict:
    """ä¸Šä¸‹æ–‡å‹ç¼©èŠ‚ç‚¹

    å·¥ä½œæµç¨‹ï¼š
    1. æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    2. å¦‚æœä¸éœ€è¦ï¼Œç›´æ¥è¿”å›ç©ºå­—å…¸
    3. å¦‚æœéœ€è¦ï¼Œè°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
    4. åˆ é™¤æ—§æ¶ˆæ¯ï¼Œä¿ç•™æœ€è¿‘æ¶ˆæ¯å’Œæ‘˜è¦
    5. è®°å½•å‹ç¼©å†å²åˆ°compression_history

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€ï¼ˆmessageså­—æ®µå’Œcompression_historyå­—æ®µï¼‰
    """
    messages = state.get("messages", [])

    print("\n" + "="*60)
    print("ğŸ—œï¸  å‹ç¼©èŠ‚ç‚¹æ‰§è¡Œ")
    print("="*60)

    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    if not needs_compression(messages):
        print("âœ… Tokenä½¿ç”¨ç‡åœ¨å®‰å…¨èŒƒå›´å†…ï¼Œæ— éœ€å‹ç¼©")
        return {}

    print("âš ï¸  Tokenä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼Œå¼€å§‹å‹ç¼©...")

    # è®°å½•å‹ç¼©å‰çš„tokenæ•°é‡
    tokens_before = get_latest_token_usage(messages)
    if tokens_before == 0:
        tokens_before = estimate_tokens(messages)

    # 2. è°ƒç”¨LLMç”Ÿæˆå‹ç¼©æ‘˜è¦
    try:
        print("ğŸ“ ç”Ÿæˆ8æ®µå¼å‹ç¼©æ‘˜è¦...")

        # æ„å»ºå‹ç¼©è¯·æ±‚
        compression_messages = [
            SystemMessage(content=COMPRESSION_PROMPT),
            *messages  # åŒ…å«å®Œæ•´å¯¹è¯å†å²
        ]

        # è°ƒç”¨LLM
        summary_response = llm.invoke(compression_messages)
        summary_content = summary_response.content

        print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(summary_content)} å­—ç¬¦")

        # 3. å†³å®šä¿ç•™å¤šå°‘æœ€è¿‘æ¶ˆæ¯
        # ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘3æ¡æ¶ˆæ¯
        keep_recent = 3

        if len(messages) <= keep_recent:
            print("âš ï¸  æ¶ˆæ¯æ•°é‡è¾ƒå°‘ï¼Œä¸è¿›è¡Œå‹ç¼©")
            return {}

        # 4. æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
        # åˆ é™¤æ—§æ¶ˆæ¯ï¼Œä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
        messages_to_remove = messages[:-keep_recent]
        recent_messages = messages[-keep_recent:]

        # å¦‚æœæ‹†åˆ†ç‚¹ä¸‹ä¸€æ¡æ˜¯ToolMessage, å°†è¯¥æ¡messageä¹Ÿåˆ é™¤
        if recent_messages and isinstance(recent_messages[0], ToolMessage):
            messages_to_remove.append(recent_messages[0])
            recent_messages = recent_messages[1:]

        # åˆ›å»ºæ‘˜è¦æ¶ˆæ¯
        summary_message = AIMessage(
            content=f"""ğŸ“‹ **å¯¹è¯æ‘˜è¦** (å‹ç¼©äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})

{summary_content}

---
*ä»¥ä¸Šæ˜¯ä¹‹å‰ {len(messages_to_remove)} æ¡æ¶ˆæ¯çš„æ‘˜è¦ï¼Œä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯*
"""
        )

        print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ¶ˆæ¯: {len(messages_to_remove)} æ¡")
        print(f"ğŸ“Œ ä¿ç•™æœ€è¿‘æ¶ˆæ¯: {len(recent_messages)} æ¡")
        print(f"ğŸ“„ æ·»åŠ æ‘˜è¦: 1 æ¡")

        # 5. è¿”å›æ›´æ–°
        # ä½¿ç”¨RemoveMessageåˆ é™¤æ—§æ¶ˆæ¯
        remove_messages = [RemoveMessage(id=msg.id) for msg in messages_to_remove if hasattr(msg, 'id')]

        # è®¡ç®—å‹ç¼©åçš„tokenæ•°é‡
        new_messages_for_count = recent_messages + [summary_message]
        tokens_after = estimate_tokens(new_messages_for_count)

        # 6. åˆ›å»ºå‹ç¼©è®°å½•
        compression_record: CompressionRecord = {
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "messages_removed": len(messages_to_remove),
            "messages_kept": len(recent_messages),
            "tokens_before": tokens_before,
            "tokens_after": tokens_after,
            "summary_content": summary_content[:200] + "..." if len(summary_content) > 200 else summary_content
        }

        # 7. è·å–ç°æœ‰çš„å‹ç¼©å†å²å¹¶æ·»åŠ æ–°è®°å½•
        compression_history = state.get("compression_history", [])

        print(f"ğŸ“Š å‹ç¼©ç»Ÿè®¡:")
        print(f"   å‹ç¼©å‰: {tokens_before:,} tokens")
        print(f"   å‹ç¼©å: {tokens_after:,} tokens")
        print(f"   èŠ‚çœ: {tokens_before - tokens_after:,} tokens ({(tokens_before - tokens_after) / tokens_before * 100:.1f}%)")

        # è¿”å›æ›´æ–°çš„å®Œæ•´åˆ—è¡¨
        return {
            "messages": remove_messages + [summary_message],
            "compression_history": compression_history + [compression_record]
        }

    except Exception as e:
        print(f"âŒ å‹ç¼©å¤±è´¥: {e}")
        print("ç»§ç»­æ‰§è¡Œï¼Œä¸è¿›è¡Œå‹ç¼©")
        return {}


# ============================================================================
# 7. æµ‹è¯•å·¥å…·å®šä¹‰
# ============================================================================

@tool
def search_docs(query: str) -> str:
    """æœç´¢æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿï¼‰

    Args:
        query: æœç´¢å…³é”®è¯
    """
    return f"æ‰¾åˆ°å…³äº'{query}'çš„æ–‡æ¡£ï¼š\n1. æ–‡æ¡£A - è¿™æ˜¯ä¸€ç¯‡å…³äº{query}çš„è¯¦ç»†ä»‹ç»...\n2. æ–‡æ¡£B - {query}çš„æœ€ä½³å®è·µ..."


@tool
def analyze_code(code: str) -> str:
    """åˆ†æä»£ç ï¼ˆæ¨¡æ‹Ÿï¼‰

    Args:
        code: è¦åˆ†æçš„ä»£ç 
    """
    return f"ä»£ç åˆ†æç»“æœï¼š\n- ä»£ç è¡Œæ•°: {len(code.split())}\n- å¤æ‚åº¦: ä¸­ç­‰\n- å»ºè®®: å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–..."


tools = [search_docs, analyze_code]
llm_with_tools = llm.bind_tools(tools)


# ============================================================================
# 8. Agent èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def agent_node(state: AgentState):
    """AgentèŠ‚ç‚¹ï¼šè°ƒç”¨LLM"""
    messages = state["messages"]

    # æ·»åŠ ç³»ç»Ÿæç¤º
    system_msg = SystemMessage(
        content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥æœç´¢æ–‡æ¡£å’Œåˆ†æä»£ç ã€‚è¯·å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚"
    )

    response = llm_with_tools.invoke([system_msg] + list(messages))
    return {"messages": [response]}


def tool_node(state: AgentState):
    """å·¥å…·èŠ‚ç‚¹ï¼šæ‰§è¡Œå·¥å…·"""
    tools_by_name = {t.name: t for t in tools}
    last_message = state["messages"][-1]

    results = []
    for tool_call in last_message.tool_calls:
        tool_func = tools_by_name[tool_call["name"]]
        result = tool_func.invoke(tool_call["args"])
        results.append(ToolMessage(
            content=str(result),
            tool_call_id=tool_call["id"]
        ))

    return {"messages": results}


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­"""
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"


# ============================================================================
# 9. æ„å»ºå›¾
# ============================================================================

def build_graph():
    """æ„å»ºå¸¦å‹ç¼©åŠŸèƒ½çš„Agentå›¾"""
    builder = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("compression", compression_node)
    builder.add_node("agent", agent_node)
    builder.add_node("tools", tool_node)

    # æ·»åŠ è¾¹
    builder.add_edge(START, "compression")
    builder.add_edge("compression", "agent")

    # agentçš„æ¡ä»¶è¾¹
    builder.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END
        }
    )

    # toolsæ‰§è¡Œåå›åˆ°compressionæ£€æŸ¥
    builder.add_edge("tools", "compression")

    # ç¼–è¯‘
    graph = builder.compile()

    print("âœ… å›¾æ„å»ºå®Œæˆ")
    return graph


# ============================================================================
# 10. æµ‹è¯•å‡½æ•°
# ============================================================================

def test_short_conversation():
    """æµ‹è¯•1ï¼šçŸ­å¯¹è¯ï¼ˆä¸è§¦å‘å‹ç¼©ï¼‰"""
    print("="*60)
    print("æµ‹è¯•1ï¼šçŸ­å¯¹è¯ - ä¸åº”è§¦å‘å‹ç¼©")
    print("="*60)

    graph = build_graph()

    result = graph.invoke({
        "messages": [HumanMessage(content="æœç´¢å…³äºLangGraphçš„æ–‡æ¡£")],
        "compression_history": []
    })

    print("\næœ€ç»ˆå›ç­”:")
    print(result["messages"][-1].content)
    print(f"\næ¶ˆæ¯æ€»æ•°: {len(result['messages'])}")
    print(f"å‹ç¼©æ¬¡æ•°: {len(result.get('compression_history', []))}")


def test_long_conversation():
    """æµ‹è¯•2ï¼šé•¿å¯¹è¯ï¼ˆè§¦å‘å‹ç¼©ï¼‰"""
    global TOKEN_THRESHOLD
    TOKEN_THRESHOLD = 0.01  # é™ä½é˜ˆå€¼ä»¥è§¦å‘å‹ç¼©

    print("\n" + "="*60)
    print("æµ‹è¯•2ï¼šé•¿å¯¹è¯æ¨¡æ‹Ÿ - åº”è§¦å‘å‹ç¼©")
    print("="*60)

    graph = build_graph()

    questions = [
        "æœç´¢LangGraphçš„åŸºç¡€æ¦‚å¿µ",
        "åˆ†æä»¥ä¸‹ä»£ç ï¼šdef hello(): pass",
        "æœç´¢StateGraphçš„ç”¨æ³•",
        "æœç´¢ToolNodeçš„å®ç°",
        "æœç´¢MessagesStateçš„å®šä¹‰",
        "æœç´¢æ¡ä»¶è¾¹çš„ä½¿ç”¨",
        "æœç´¢interruptæœºåˆ¶",
        "æœç´¢checkpointerçš„é…ç½®"
    ]

    state = {"messages": [], "compression_history": []}

    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"ç¬¬ {i} è½®å¯¹è¯: {question}")
        print(f"{'='*60}")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        state["messages"] = list(state["messages"]) + [HumanMessage(content=question)]

        # æ‰§è¡Œ
        result = graph.invoke(state)
        state = result

        print(f"\nå½“å‰æ¶ˆæ¯æ•°: {len(state['messages'])}")
        print(f"å‹ç¼©æ¬¡æ•°: {len(state.get('compression_history', []))}")

    print("\n" + "="*60)
    print("é•¿å¯¹è¯æµ‹è¯•å®Œæˆ")
    print("="*60)
    print(f"æœ€ç»ˆæ¶ˆæ¯æ•°: {len(state['messages'])}")
    print(f"æ€»å‹ç¼©æ¬¡æ•°: {len(state.get('compression_history', []))}")

    # æ˜¾ç¤ºå‹ç¼©å†å²æ‘˜è¦
    if state.get('compression_history'):
        print("\nå‹ç¼©å†å²æ‘˜è¦:")
        for i, record in enumerate(state['compression_history'], 1):
            print(f"\nç¬¬ {i} æ¬¡å‹ç¼©:")
            print(f"  æ—¶é—´: {record['timestamp']}")
            print(f"  åˆ é™¤æ¶ˆæ¯: {record['messages_removed']} æ¡")
            print(f"  ä¿ç•™æ¶ˆæ¯: {record['messages_kept']} æ¡")
            saved = record['tokens_before'] - record['tokens_after']
            ratio = (saved / record['tokens_before'] * 100) if record['tokens_before'] > 0 else 0
            print(f"  èŠ‚çœ: {saved:,} tokens ({ratio:.1f}%)")


# ============================================================================
# 11. ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•1ï¼šçŸ­å¯¹è¯
    # test_short_conversation()

    # æµ‹è¯•2ï¼šé•¿å¯¹è¯
    test_long_conversation()


if __name__ == "__main__":
    main()
