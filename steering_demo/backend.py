"""
Streaming + Steering Demo - åç«¯å®ç°
ä½¿ç”¨ FastAPI + LangGraph å®ç°çœŸæ­£çš„æµå¼è¾“å‡ºå’Œä¸­æ–­æ§åˆ¶
"""
import os
import time
import asyncio
import uuid
from typing import AsyncGenerator, Dict, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# LangGraph å’Œ LangChain
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage

# LLM - æ ¹æ®ä½ çš„ç¯å¢ƒé€‰æ‹©

from langchain_community.chat_models import ChatTongyi
llm = ChatTongyi(model="qwen3-max", temperature=0)

print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ: {llm.model_name}")


# ========== å·¥å…·å®šä¹‰ ==========
@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿ2ç§’å»¶è¿Ÿï¼‰"""
    for i in range(10):
        print(f"ğŸ” æœç´¢: {query} {i}")
        time.sleep(1)
    return f"æ‰¾åˆ°å…³äº '{query}' çš„ 3 æ¡ç»“æœï¼šç»“æœ1ã€ç»“æœ2ã€ç»“æœ3"


@tool
def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ï¼ˆæ¨¡æ‹Ÿ1ç§’å»¶è¿Ÿï¼‰"""
    print(f"ğŸ§® è®¡ç®—: {expression}")
    try:
        for i in range(20):
            print(f"ğŸ§® è®¡ç®—: {expression} {i}")
            time.sleep(0.5)
        result = eval(expression)
        return f"{expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


@tool
def fetch_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”ï¼ˆæ¨¡æ‹Ÿ1.5ç§’å»¶è¿Ÿï¼‰"""
    print(f"ğŸŒ¤ï¸ è·å–å¤©æ°”: {city}")
    time.sleep(1.5)
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œæ¸©åº¦ 25Â°C",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œæ¸©åº¦ 28Â°C",
        "æ·±åœ³": "é˜´å¤©ï¼Œæ¸©åº¦ 30Â°C"
    }
    return weather_data.get(city, f"{city}ï¼šæ™´å¤©ï¼Œæ¸©åº¦ 22Â°C")


# å·¥å…·åˆ—è¡¨
tools = [search_database, calculate, fetch_weather]


# ========== Agent å…¨å±€å˜é‡ ==========
checkpointer = MemorySaver()
agent = create_react_agent(llm, tools=tools, checkpointer=checkpointer)

# å­˜å‚¨æ¯ä¸ªä¼šè¯çš„ä¸­æ–­æ ‡å¿—
abort_flags: Dict[str, bool] = {}


# ========== è¯·æ±‚æ¨¡å‹ ==========
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None  # ä¿®æ”¹ä¸º Optionalï¼Œæ­£ç¡®å¤„ç† null


class AbortRequest(BaseModel):
    session_id: str


# ========== FastAPI åº”ç”¨ ==========
app = FastAPI(title="Streaming + Steering Demo")

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "running", "message": "Streaming + Steering Demo Backend"}


@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    æµå¼èŠå¤©ç«¯ç‚¹ï¼ˆä½¿ç”¨ SSEï¼‰
    æ”¯æŒä¸­æ–­å’Œæ¢å¤
    """
    print(f"æ”¶åˆ°è¯·æ±‚: {request}")
    print(f"æ¶ˆæ¯: {request.message}")
    print(f"ä¼šè¯ID: {request.session_id}")

    # ç”Ÿæˆæˆ–ä½¿ç”¨ç°æœ‰çš„ session_id
    session_id = request.session_id or str(uuid.uuid4())

    # åˆå§‹åŒ–ä¸­æ–­æ ‡å¿—
    abort_flags[session_id] = False

    print(f"\n{'='*60}")
    print(f"[ä¼šè¯ {session_id[:8]}] æ–°æ¶ˆæ¯: {request.message}")
    print(f"{'='*60}\n")

    async def event_generator() -> AsyncGenerator[str, None]:
        """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
        try:
            # é…ç½®
            config = {
                "configurable": {"thread_id": session_id}
            }

            # ğŸ”‘ æ£€æŸ¥æ˜¯å¦æœ‰ pending çš„ tool_callsï¼ˆé˜²æ­¢çŠ¶æ€ä¸ä¸€è‡´ï¼‰
            current_state = agent.get_state(config)
            if current_state.next and 'tools' in current_state.next:
                print(f"[ä¼šè¯ {session_id[:8]}] âš ï¸ æ£€æµ‹åˆ° pending çš„ tool_callsï¼Œå…ˆå®Œæˆå®ƒä»¬")

                # è®© pending çš„ tool_calls æ‰§è¡Œå®Œæˆ
                async for msg, metadata in agent.astream(None, config, stream_mode="messages"):
                    if isinstance(msg, AIMessage) and msg.content:
                        yield f"event: token\ndata: {msg.content}\n\n"
                        asyncio.sleep(0.1) # å¢å¼ºæ¼”ç¤ºæ•ˆæœ
                print(f"[ä¼šè¯ {session_id[:8]}] âœ… Pending tool_calls å·²å®Œæˆ")

            # è¾“å…¥æ¶ˆæ¯
            input_msg = {"messages": [HumanMessage(content=request.message)]}

            # å‘é€ä¼šè¯ ID
            yield f"event: session_id\ndata: {session_id}\n\n"

            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"event: start\ndata: å¼€å§‹å¤„ç†...\n\n"

            # æµå¼æ‰§è¡Œ Agent
            async for msg, metadata in agent.astream(
                input_msg, config, stream_mode="messages"
            ):
                # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
                if abort_flags.get(session_id, False):
                    print(f"[ä¼šè¯ {session_id[:8]}] ğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·")
                    yield f"event: aborted\ndata: æ‰§è¡Œå·²ä¸­æ–­\n\n"
                    break

                # åªå‘é€ AI çš„å›å¤å†…å®¹
                if isinstance(msg, AIMessage) and msg.content:
                    # å‘é€ token
                    yield f"event: token\ndata: {msg.content}\n\n"
                    await asyncio.sleep(0.1) # å¢å¼ºæ¼”ç¤ºæ•ˆæœ

            # å‘é€å®Œæˆäº‹ä»¶
            if not abort_flags.get(session_id, False):
                yield f"event: done\ndata: å®Œæˆ\n\n"
                print(f"[ä¼šè¯ {session_id[:8]}] âœ… å®Œæˆ")

        except Exception as e:
            print(f"[ä¼šè¯ {session_id[:8]}] âŒ é”™è¯¯: {e}")
            yield f"event: error\ndata: {str(e)}\n\n"

        finally:
            # æ¸…ç†ä¸­æ–­æ ‡å¿—
            if session_id in abort_flags:
                del abort_flags[session_id]

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        }
    )


@app.post("/api/abort")
async def abort_chat(request: AbortRequest):
    """
    ä¸­æ–­å½“å‰æ‰§è¡Œ
    """
    session_id = request.session_id

    if session_id in abort_flags:
        abort_flags[session_id] = True
        print(f"[ä¼šè¯ {session_id[:8]}] ğŸ›‘ æ”¶åˆ°ä¸­æ–­è¯·æ±‚")
        return {"status": "success", "message": "ä¸­æ–­ä¿¡å·å·²å‘é€"}
    else:
        return {"status": "not_found", "message": "ä¼šè¯ä¸å­˜åœ¨æˆ–å·²ç»“æŸ"}


@app.get("/api/history/{session_id}")
async def get_history(session_id: str):
    """
    è·å–ä¼šè¯å†å²
    """
    try:
        config = {"configurable": {"thread_id": session_id}}
        state = agent.get_state(config)

        messages = []
        for msg in state.values.get("messages", []):
            messages.append({
                "role": "human" if isinstance(msg, HumanMessage) else "ai",
                "content": msg.content
            })

        return {
            "status": "success",
            "session_id": session_id,
            "messages": messages,
            "message_count": len(messages)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/session/{session_id}")
async def clear_session(session_id: str):
    """
    æ¸…é™¤ä¼šè¯å†å²
    """
    # æ³¨æ„ï¼šMemorySaver æ²¡æœ‰ç›´æ¥çš„åˆ é™¤æ–¹æ³•
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ä½¿ç”¨ RedisSaver æˆ–å…¶ä»–æ”¯æŒåˆ é™¤çš„å­˜å‚¨
    return {
        "status": "success",
        "message": "ä¼šè¯æ¸…é™¤è¯·æ±‚å·²æ¥æ”¶ï¼ˆMemorySaver æš‚ä¸æ”¯æŒåˆ é™¤ï¼‰"
    }


if __name__ == "__main__":
    import uvicorn

    print("\n" + "="*60)
    print("ğŸš€ Streaming + Steering Demo åç«¯å¯åŠ¨")
    print("="*60)
    print(f"LLM: {llm.__class__.__name__}")
    print(f"å·¥å…·æ•°é‡: {len(tools)}")
    print("API ç«¯ç‚¹:")
    print("  - POST /api/chat        - æµå¼èŠå¤©")
    print("  - POST /api/abort       - ä¸­æ–­æ‰§è¡Œ")
    print("  - GET  /api/history/:id - è·å–å†å²")
    print("="*60)
    print("\nç›‘å¬åœ°å€: http://localhost:8000")
    print("å‰ç«¯é¡µé¢: è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ frontend.html\n")

    uvicorn.run(app, host="0.0.0.0", port=8000)
